{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28666e4f",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Methods Of Extractive Text Summarization</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3108e1",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Preparation</p>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd3d8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wjsci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wjsci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Installing libraries.\n",
    "#%pip install rouge_score\n",
    "\n",
    "# Imports.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import operator\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08541d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datasets.\n",
    "test = pd.read_csv('test.csv')\n",
    "test.to_pickle(\"./test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da137756",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Summarization Parameters</p>\n",
    "---\n",
    "Sentence count refers to the amount of sentences that each method will select for the purpose of extractive text summarization. Increasing this number will generate longer summaries. The evaluation of each method involves the ROUGE-1 standard, which compares unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd70a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary parameters.\n",
    "max_sentence_count = 2\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e36df",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Pre-Processing</p>\n",
    "---\n",
    "The following function details the text cleaning process used on sentence strings before they are scored. It involves the removal of stopwords and punctuation, the use of a porter stemmer, and the decapitalization of every character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da62aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    # setting stop words.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    # creating stemmer.\n",
    "    porter = PorterStemmer()\n",
    "    # remove punctuation.\n",
    "    for punc in string.punctuation:\n",
    "        data = data.replace(punc, '')\n",
    "    data = data.replace('‘', '')\n",
    "    data = data.replace('’', '')\n",
    "    data = data.replace('“', '')\n",
    "    # remove numbers\n",
    "    data = re.sub(r'\\d+','NUM',data)\n",
    "    # remove non ascii characters.\n",
    "    chars = set(string.printable)\n",
    "    filter(lambda x: x in chars, data)\n",
    "    # convert to lower case.\n",
    "    data = data.lower()\n",
    "    # tokenizing.\n",
    "    tokens = word_tokenize(data)\n",
    "    # removing stop words.\n",
    "    filteredTokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stops:\n",
    "            filteredTokens.append(token)\n",
    "    # stemming.\n",
    "    for i in range (len(filteredTokens)):\n",
    "        # stem.\n",
    "        filteredTokens[i] = porter.stem(filteredTokens[i])\n",
    "    # recreating string.\n",
    "    message = ' '\n",
    "    message = message.join(filteredTokens)\n",
    "    # returning.\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7d955",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Baseline</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">Random Selection</p>\n",
    "---\n",
    "\n",
    "This method represents a baseline with which to compare the following methods to. It selects K random sentences from each article to represent the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d6fd59b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2941267710810191\n",
      "Average Recall: 0.24796226056831208\n",
      "Average FMeasure: 0.2529957733977247\n",
      "r2\n",
      "Average Precision: 0.0822413232214134\n",
      "Average Recall: 0.07022443574955406\n",
      "Average FMeasure: 0.07096243276214713\n",
      "rL\n",
      "Average Precision: 0.1904044432022451\n",
      "Average Recall: 0.158891988998144\n",
      "Average FMeasure: 0.1622936651470126\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29702421189954664\n",
      "Average Recall: 0.24886382943219473\n",
      "Average FMeasure: 0.2556097980660546\n",
      "r2\n",
      "Average Precision: 0.08370922583913025\n",
      "Average Recall: 0.07098957503701919\n",
      "Average FMeasure: 0.07256688903585656\n",
      "rL\n",
      "Average Precision: 0.19197777325288998\n",
      "Average Recall: 0.15931205610033664\n",
      "Average FMeasure: 0.16400349400039482\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29712555664453094\n",
      "Average Recall: 0.24750256935270923\n",
      "Average FMeasure: 0.2549456326387416\n",
      "r2\n",
      "Average Precision: 0.08273229840576247\n",
      "Average Recall: 0.06946926882140182\n",
      "Average FMeasure: 0.07119002700035389\n",
      "rL\n",
      "Average Precision: 0.1917665117672709\n",
      "Average Recall: 0.15809263085254938\n",
      "Average FMeasure: 0.16315542985258236\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29525897791742245\n",
      "Average Recall: 0.24761386845767583\n",
      "Average FMeasure: 0.2545390201291382\n",
      "r2\n",
      "Average Precision: 0.08127782455973177\n",
      "Average Recall: 0.06862883692702247\n",
      "Average FMeasure: 0.07023911326752552\n",
      "rL\n",
      "Average Precision: 0.18997294226970857\n",
      "Average Recall: 0.15757942786852538\n",
      "Average FMeasure: 0.16239470831507455\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29422787906263453\n",
      "Average Recall: 0.2469739311608825\n",
      "Average FMeasure: 0.2537511169019367\n",
      "r2\n",
      "Average Precision: 0.08138205951091444\n",
      "Average Recall: 0.06883872449283794\n",
      "Average FMeasure: 0.07039778451963415\n",
      "rL\n",
      "Average Precision: 0.18967449659376592\n",
      "Average Recall: 0.15770656604638042\n",
      "Average FMeasure: 0.1623388383006879\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2947722974724439\n",
      "Average Recall: 0.24671335237768607\n",
      "Average FMeasure: 0.2536234053063293\n",
      "r2\n",
      "Average Precision: 0.08164333361894649\n",
      "Average Recall: 0.06868360939209715\n",
      "Average FMeasure: 0.070374871350942\n",
      "rL\n",
      "Average Precision: 0.1902690443259791\n",
      "Average Recall: 0.1577623805456841\n",
      "Average FMeasure: 0.16252456208120464\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2961058965587603\n",
      "Average Recall: 0.24709805432573848\n",
      "Average FMeasure: 0.2542979358249938\n",
      "r2\n",
      "Average Precision: 0.08236051883105958\n",
      "Average Recall: 0.06889980219308561\n",
      "Average FMeasure: 0.07074634205008953\n",
      "rL\n",
      "Average Precision: 0.19107966955603067\n",
      "Average Recall: 0.15808110670592407\n",
      "Average FMeasure: 0.16297840671974775\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2946716042229803\n",
      "Average Recall: 0.24695773238472038\n",
      "Average FMeasure: 0.2537212668143405\n",
      "r2\n",
      "Average Precision: 0.08170866285728681\n",
      "Average Recall: 0.06872634704059666\n",
      "Average FMeasure: 0.0704209084515523\n",
      "rL\n",
      "Average Precision: 0.1899105490090604\n",
      "Average Recall: 0.15791342183909082\n",
      "Average FMeasure: 0.16245316739938265\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29483856858756446\n",
      "Average Recall: 0.24704226968215617\n",
      "Average FMeasure: 0.2538680671386185\n",
      "r2\n",
      "Average Precision: 0.08135524435716748\n",
      "Average Recall: 0.06858006395116774\n",
      "Average FMeasure: 0.07022347276448673\n",
      "rL\n",
      "Average Precision: 0.1898240870916531\n",
      "Average Recall: 0.15779572725811047\n",
      "Average FMeasure: 0.16236663611490135\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.2950110133748159\n",
      "Average Recall: 0.24685927922670572\n",
      "Average FMeasure: 0.25391416150923385\n",
      "r2\n",
      "Average Precision: 0.08113016301592127\n",
      "Average Recall: 0.06835374967197089\n",
      "Average FMeasure: 0.07004734345855264\n",
      "rL\n",
      "Average Precision: 0.18973217849101084\n",
      "Average Recall: 0.15738261308113854\n",
      "Average FMeasure: 0.1621473172633448\n"
     ]
    }
   ],
   "source": [
    "def randomized():    \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary and sentence scores.\n",
    "        summary = []\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Getting random sentences.\n",
    "        sentence_ids = random.sample(range(0, len(sentences)), sentence_count)\n",
    "        for sentence_id in sentence_ids:\n",
    "            summary.append(sentences[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "\n",
    "# Run.\n",
    "randomized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d77a10",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 1</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF Combination</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. In this case, each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, the word scores for each sentence/document are summed together in order to obtain a final per sentence scoring. This does not occur per word token in each sentence, but rather involves summing the |V| values for each word in the vocabulary per document. In this way, sentences that contain more unusual words will tend to score higher and be selected for summarization. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d615a8f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2599599147054455\n",
      "Average Recall: 0.39867837836593795\n",
      "Average FMeasure: 0.30270692852086983\n",
      "r2\n",
      "Average Precision: 0.08623954812325758\n",
      "Average Recall: 0.12764525932259554\n",
      "Average FMeasure: 0.09870898742057657\n",
      "rL\n",
      "Average Precision: 0.15768006827309142\n",
      "Average Recall: 0.2430350554041168\n",
      "Average FMeasure: 0.18361663105652826\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.26066740456421605\n",
      "Average Recall: 0.39651511283743296\n",
      "Average FMeasure: 0.30277339566404765\n",
      "r2\n",
      "Average Precision: 0.08505318410980937\n",
      "Average Recall: 0.1254232644690386\n",
      "Average FMeasure: 0.0972175336357825\n",
      "rL\n",
      "Average Precision: 0.15649400887661932\n",
      "Average Recall: 0.23994061952108134\n",
      "Average FMeasure: 0.18205907764620052\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.26153421045458375\n",
      "Average Recall: 0.3967156816556744\n",
      "Average FMeasure: 0.3036787167525011\n",
      "r2\n",
      "Average Precision: 0.08506058077837579\n",
      "Average Recall: 0.12521071299751027\n",
      "Average FMeasure: 0.09733461378926134\n",
      "rL\n",
      "Average Precision: 0.1567550031386268\n",
      "Average Recall: 0.23993260771062763\n",
      "Average FMeasure: 0.18246710226772045\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2598433659070738\n",
      "Average Recall: 0.3971192265018156\n",
      "Average FMeasure: 0.3025868615306491\n",
      "r2\n",
      "Average Precision: 0.08365840799772005\n",
      "Average Recall: 0.12419460847573577\n",
      "Average FMeasure: 0.09607146409680892\n",
      "rL\n",
      "Average Precision: 0.15513903949091037\n",
      "Average Recall: 0.23940535486847234\n",
      "Average FMeasure: 0.18117214972917592\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2588072342313829\n",
      "Average Recall: 0.3955483964953064\n",
      "Average FMeasure: 0.30154366065662086\n",
      "r2\n",
      "Average Precision: 0.0834377027450854\n",
      "Average Recall: 0.12374762177248597\n",
      "Average FMeasure: 0.09582978745191881\n",
      "rL\n",
      "Average Precision: 0.15476816738375618\n",
      "Average Recall: 0.23863627131436532\n",
      "Average FMeasure: 0.18078482315101266\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.259625974176206\n",
      "Average Recall: 0.3964048157616045\n",
      "Average FMeasure: 0.3022940764436377\n",
      "r2\n",
      "Average Precision: 0.08410229038087208\n",
      "Average Recall: 0.1242893775151471\n",
      "Average FMeasure: 0.09643732710134845\n",
      "rL\n",
      "Average Precision: 0.15533286986468892\n",
      "Average Recall: 0.23892637523086413\n",
      "Average FMeasure: 0.1812160489586557\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.26014241570204855\n",
      "Average Recall: 0.39602288003807884\n",
      "Average FMeasure: 0.30239300381729056\n",
      "r2\n",
      "Average Precision: 0.08401567328820654\n",
      "Average Recall: 0.12370636066745984\n",
      "Average FMeasure: 0.09612596522434838\n",
      "rL\n",
      "Average Precision: 0.15547698962334056\n",
      "Average Recall: 0.238479471760557\n",
      "Average FMeasure: 0.18108521922139129\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25952876383523676\n",
      "Average Recall: 0.39623427659180954\n",
      "Average FMeasure: 0.30219161279533824\n",
      "r2\n",
      "Average Precision: 0.08371523558256777\n",
      "Average Recall: 0.12353459911146386\n",
      "Average FMeasure: 0.09593219910181085\n",
      "rL\n",
      "Average Precision: 0.15499463090815785\n",
      "Average Recall: 0.2383743724772018\n",
      "Average FMeasure: 0.1808234151726619\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25923970166417337\n",
      "Average Recall: 0.39556062548240084\n",
      "Average FMeasure: 0.30186329456080524\n",
      "r2\n",
      "Average Precision: 0.0835834727049195\n",
      "Average Recall: 0.12324738113251818\n",
      "Average FMeasure: 0.09578687792527352\n",
      "rL\n",
      "Average Precision: 0.15477565968820886\n",
      "Average Recall: 0.23776042233150924\n",
      "Average FMeasure: 0.18053355364142168\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.25955565361460825\n",
      "Average Recall: 0.3951000683726107\n",
      "Average FMeasure: 0.3020152478693879\n",
      "r2\n",
      "Average Precision: 0.08359015325867958\n",
      "Average Recall: 0.12280287408602242\n",
      "Average FMeasure: 0.09567570237522469\n",
      "rL\n",
      "Average Precision: 0.15491831201549247\n",
      "Average Recall: 0.23740411226302127\n",
      "Average FMeasure: 0.1805645479738578\n"
     ]
    }
   ],
   "source": [
    "def tfidfcombo():    \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary and sentence scores.\n",
    "        summary = []\n",
    "        id2score = {}\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores.\n",
    "        tf_idf = vectorizer.fit_transform(sentences).todense()\n",
    "        # Sum each sentence's |V| TF-IDF values.\n",
    "        for i in range(tf_idf.shape[0]):\n",
    "            score = 0\n",
    "            for j in range(tf_idf.shape[1]):\n",
    "                score += tf_idf[i,j]\n",
    "            id2score[i] = score\n",
    "        # Selecting K highest-scoring sentences for summarization.\n",
    "        for k in range(sentence_count):\n",
    "            sentence_id = max(id2score.items(), key=operator.itemgetter(1))[0]\n",
    "            id2score.pop(sentence_id)\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "\n",
    "# Run.\n",
    "tfidfcombo()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27729fd4",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 2</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF Summation</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. In this case, each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, the word scores for each word in each sentence/document are summed together in order to obtain a final per sentence scoring. This is a slightly different approach compared with method one, because we sum scores for each word token in a sentence rather than summing the |V| TF-IDF scores in the TF-IDF matrix per sentence. This has the downside of immediately giving longer sentences a more positive value, skewing recall and precision. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8b7cd655",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2533201965235825\n",
      "Average Recall: 0.38492604092016025\n",
      "Average FMeasure: 0.2934175761285802\n",
      "r2\n",
      "Average Precision: 0.08267410308629339\n",
      "Average Recall: 0.12138264846699072\n",
      "Average FMeasure: 0.09423580309800486\n",
      "rL\n",
      "Average Precision: 0.15389022111327263\n",
      "Average Recall: 0.23475141396068475\n",
      "Average FMeasure: 0.17818600657196224\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2526791277968233\n",
      "Average Recall: 0.38075564808044876\n",
      "Average FMeasure: 0.29209448847152564\n",
      "r2\n",
      "Average Precision: 0.08123949457998794\n",
      "Average Recall: 0.11873973914031216\n",
      "Average FMeasure: 0.09249167584541106\n",
      "rL\n",
      "Average Precision: 0.15315458473822477\n",
      "Average Recall: 0.23213265548157241\n",
      "Average FMeasure: 0.177166862287821\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25352806507869363\n",
      "Average Recall: 0.38064877830578986\n",
      "Average FMeasure: 0.2928056882861603\n",
      "r2\n",
      "Average Precision: 0.0812244901699128\n",
      "Average Recall: 0.11840221488613908\n",
      "Average FMeasure: 0.0925048665570376\n",
      "rL\n",
      "Average Precision: 0.15351384102605978\n",
      "Average Recall: 0.23209249590720624\n",
      "Average FMeasure: 0.17757520406359056\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2520122616801446\n",
      "Average Recall: 0.3810822444697381\n",
      "Average FMeasure: 0.2918901022875859\n",
      "r2\n",
      "Average Precision: 0.079827850007238\n",
      "Average Recall: 0.11736740000120298\n",
      "Average FMeasure: 0.09123703625200445\n",
      "rL\n",
      "Average Precision: 0.15193243135936957\n",
      "Average Recall: 0.23150627200324747\n",
      "Average FMeasure: 0.17631335961179181\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2514728934555504\n",
      "Average Recall: 0.38086638477143664\n",
      "Average FMeasure: 0.291625687185565\n",
      "r2\n",
      "Average Precision: 0.0795537653188591\n",
      "Average Recall: 0.11707764840217633\n",
      "Average FMeasure: 0.09100813150093422\n",
      "rL\n",
      "Average Precision: 0.1515746197032229\n",
      "Average Recall: 0.23126249458146048\n",
      "Average FMeasure: 0.17609425556599237\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25279090581372526\n",
      "Average Recall: 0.3825109012369203\n",
      "Average FMeasure: 0.2929594047136934\n",
      "r2\n",
      "Average Precision: 0.08057281620255952\n",
      "Average Recall: 0.11814854312558254\n",
      "Average FMeasure: 0.09201106958721637\n",
      "rL\n",
      "Average Precision: 0.1522110337006811\n",
      "Average Recall: 0.2319287733528699\n",
      "Average FMeasure: 0.1767023505578266\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25359525928730126\n",
      "Average Recall: 0.3827697326448122\n",
      "Average FMeasure: 0.2934266214721028\n",
      "r2\n",
      "Average Precision: 0.08090008734776889\n",
      "Average Recall: 0.11823134612793733\n",
      "Average FMeasure: 0.09219836189616268\n",
      "rL\n",
      "Average Precision: 0.15268277137980246\n",
      "Average Recall: 0.2321484885573678\n",
      "Average FMeasure: 0.1769856698332067\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2527960526110699\n",
      "Average Recall: 0.3830234991963751\n",
      "Average FMeasure: 0.29306942815912596\n",
      "r2\n",
      "Average Precision: 0.0804248739908981\n",
      "Average Recall: 0.11794150599785515\n",
      "Average FMeasure: 0.09183370391428192\n",
      "rL\n",
      "Average Precision: 0.15217899673919194\n",
      "Average Recall: 0.23217733811266408\n",
      "Average FMeasure: 0.17673400277610155\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.25240718839941406\n",
      "Average Recall: 0.3825110407817141\n",
      "Average FMeasure: 0.2927288992025058\n",
      "r2\n",
      "Average Precision: 0.08035389066388862\n",
      "Average Recall: 0.11798726014490163\n",
      "Average FMeasure: 0.09182695944117925\n",
      "rL\n",
      "Average Precision: 0.15210535805929787\n",
      "Average Recall: 0.23197544935397926\n",
      "Average FMeasure: 0.17667127439785277\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.2527420008884324\n",
      "Average Recall: 0.3824063269085154\n",
      "Average FMeasure: 0.2930154543184295\n",
      "r2\n",
      "Average Precision: 0.08050826666615722\n",
      "Average Recall: 0.11789265520092199\n",
      "Average FMeasure: 0.09193389831694326\n",
      "rL\n",
      "Average Precision: 0.1523410406828045\n",
      "Average Recall: 0.2319477702857919\n",
      "Average FMeasure: 0.17687915617278807\n"
     ]
    }
   ],
   "source": [
    "def tfidfsum():    \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary and sentence scores.\n",
    "        summary = []\n",
    "        id2score = {}\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Get average sentence length.\n",
    "        avg_length = 0\n",
    "        for sentence in sentences:\n",
    "            avg_length += len(word_tokenize(sentence))\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores\n",
    "        tf_idf = vectorizer.fit_transform(sentences).todense()\n",
    "        # Get feature names (words).\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        # Sum each word's TF-IDF values per sentence, then average across sentence length.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            words = word_tokenize(sentence)\n",
    "            length = len(words)\n",
    "            for word in words:\n",
    "                score += tf_idf[i, np.where(feature_names == word)]\n",
    "            # Apply final penalty.\n",
    "            penalty = 0\n",
    "            #penalty = abs((min(length, avg_length) - avg_length) / avg_length)**1.5\n",
    "            #penalty = abs((length - avg_length) / avg_length)**2\n",
    "            id2score[i] = (score - penalty)\n",
    "        # Selecting K highest-scoring sentences for summarization.\n",
    "        for k in range(sentence_count):\n",
    "            sentence_id = max(id2score.items(), key=operator.itemgetter(1))[0]\n",
    "            id2score.pop(sentence_id)\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "\n",
    "# Run.\n",
    "tfidfsum()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1411e7f",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 3</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF Averaging</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. In this case, each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, the word scores for each word in each sentence/document are summed together and then averaged in order to obtain a final per sentence scoring. This is a slightly different approach compared with method one, because we sum scores for each word token in a sentence rather than summing the |V| TF-IDF scores in the TF-IDF matrix per sentence. The averaging helps to avoid the bias of choosing longer sentences, while a penalty term helps to avoid choosing sentences that are too short. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "91eedd5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2786637482263132\n",
      "Average Recall: 0.249010544689199\n",
      "Average FMeasure: 0.2512942160039919\n",
      "r2\n",
      "Average Precision: 0.07556382490606885\n",
      "Average Recall: 0.06686577850004738\n",
      "Average FMeasure: 0.06735130835812841\n",
      "rL\n",
      "Average Precision: 0.1767646588599449\n",
      "Average Recall: 0.15837511430074977\n",
      "Average FMeasure: 0.1594186813898363\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27774485723797276\n",
      "Average Recall: 0.24665597390406174\n",
      "Average FMeasure: 0.25047471670146304\n",
      "r2\n",
      "Average Precision: 0.07344743846525444\n",
      "Average Recall: 0.06486092064009288\n",
      "Average FMeasure: 0.06572787572578308\n",
      "rL\n",
      "Average Precision: 0.17560982502932637\n",
      "Average Recall: 0.1567312224855122\n",
      "Average FMeasure: 0.15860384789610785\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27676853346914804\n",
      "Average Recall: 0.24499243469021323\n",
      "Average FMeasure: 0.24940950485988556\n",
      "r2\n",
      "Average Precision: 0.07280643386814516\n",
      "Average Recall: 0.06389101716041289\n",
      "Average FMeasure: 0.06504667560682234\n",
      "rL\n",
      "Average Precision: 0.17483213000094755\n",
      "Average Recall: 0.15567789360018955\n",
      "Average FMeasure: 0.1578639894517331\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27453131831601524\n",
      "Average Recall: 0.24477392916806204\n",
      "Average FMeasure: 0.2482530297035666\n",
      "r2\n",
      "Average Precision: 0.07200026718405028\n",
      "Average Recall: 0.0637229011674528\n",
      "Average FMeasure: 0.06461964418785301\n",
      "rL\n",
      "Average Precision: 0.1737737626030246\n",
      "Average Recall: 0.15586033725328893\n",
      "Average FMeasure: 0.15747369259062147\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.273480184901226\n",
      "Average Recall: 0.24387121866400152\n",
      "Average FMeasure: 0.24744789801616476\n",
      "r2\n",
      "Average Precision: 0.07149940505929227\n",
      "Average Recall: 0.06306633754763899\n",
      "Average FMeasure: 0.06411706541463505\n",
      "rL\n",
      "Average Precision: 0.17293956789624104\n",
      "Average Recall: 0.15508309621167637\n",
      "Average FMeasure: 0.15680797280045963\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2742478858188015\n",
      "Average Recall: 0.24362454904455502\n",
      "Average FMeasure: 0.24757033927668473\n",
      "r2\n",
      "Average Precision: 0.07219064564808333\n",
      "Average Recall: 0.0632569294914399\n",
      "Average FMeasure: 0.06448158176634515\n",
      "rL\n",
      "Average Precision: 0.17365847780611757\n",
      "Average Recall: 0.155096349273405\n",
      "Average FMeasure: 0.1570825419172769\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27503878566476936\n",
      "Average Recall: 0.24354350106320485\n",
      "Average FMeasure: 0.2477637718551206\n",
      "r2\n",
      "Average Precision: 0.0724457021498251\n",
      "Average Recall: 0.06339191979936729\n",
      "Average FMeasure: 0.06466961833601154\n",
      "rL\n",
      "Average Precision: 0.1742406646681268\n",
      "Average Recall: 0.15512429897359406\n",
      "Average FMeasure: 0.15726705647305253\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27474521593869533\n",
      "Average Recall: 0.24377799048449109\n",
      "Average FMeasure: 0.24792677950891454\n",
      "r2\n",
      "Average Precision: 0.07261859446958277\n",
      "Average Recall: 0.0637402040626068\n",
      "Average FMeasure: 0.0649781245715117\n",
      "rL\n",
      "Average Precision: 0.17420452111817075\n",
      "Average Recall: 0.15540385430439177\n",
      "Average FMeasure: 0.15750860063132804\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.27535805995486495\n",
      "Average Recall: 0.24420259024781843\n",
      "Average FMeasure: 0.24844402008818084\n",
      "r2\n",
      "Average Precision: 0.07307898000439024\n",
      "Average Recall: 0.06416415722561775\n",
      "Average FMeasure: 0.06540134795958791\n",
      "rL\n",
      "Average Precision: 0.17454337054234215\n",
      "Average Recall: 0.1556426898844408\n",
      "Average FMeasure: 0.1578047599456805\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.274988686948401\n",
      "Average Recall: 0.24346746755535337\n",
      "Average FMeasure: 0.24792805713846136\n",
      "r2\n",
      "Average Precision: 0.07279714166782558\n",
      "Average Recall: 0.06378490866080455\n",
      "Average FMeasure: 0.06509529194767566\n",
      "rL\n",
      "Average Precision: 0.17433421402300445\n",
      "Average Recall: 0.1551801141675776\n",
      "Average FMeasure: 0.15749195284104736\n"
     ]
    }
   ],
   "source": [
    "def tfidfavg():    \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary and sentence scores.\n",
    "        summary = []\n",
    "        id2score = {}\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Get average sentence length.\n",
    "        avg_length = 0\n",
    "        for sentence in sentences:\n",
    "            avg_length += len(word_tokenize(sentence))\n",
    "        avg_length /= len(sentences)\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores\n",
    "        tf_idf = vectorizer.fit_transform(sentences).todense()\n",
    "        # Get feature names (words).\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        # Sum each word's TF-IDF values per sentence, then average across sentence length.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            words = word_tokenize(sentence)\n",
    "            length = len(words)\n",
    "            for word in words:\n",
    "                score += tf_idf[i, np.where(feature_names == word)]\n",
    "            score /= length\n",
    "            # Apply final penalty.\n",
    "            #penalty = 0\n",
    "            penalty = abs((min(length, avg_length) - avg_length) / avg_length)**1.5\n",
    "            #penalty = abs((length - avg_length) / avg_length)**2\n",
    "            id2score[i] = (score - penalty)\n",
    "        # Selecting K highest-scoring sentences for summarization.\n",
    "        for k in range(sentence_count):\n",
    "            sentence_id = max(id2score.items(), key=operator.itemgetter(1))[0]\n",
    "            id2score.pop(sentence_id)\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "\n",
    "# Run.\n",
    "tfidfavg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221636a",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 4</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF Non-negative Matrix Factorization Summation</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. Each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, non-negative matrix factorization can be used to reduce the matrix into two matrices of lesser dimensionality of size |D| x F and F x |V| where F is the number of latent features present in the article. Each of the |D| rows in the |D| x F matrix can be summed together in order to obtain a final per sentence scoring. In this way, sentences that generalize well over the core features of the article will be selected for summarization. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "87d471fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3109971302293935\n",
      "Average Recall: 0.294051461080157\n",
      "Average FMeasure: 0.28537794048078113\n",
      "r2\n",
      "Average Precision: 0.10469410700970007\n",
      "Average Recall: 0.09816019647448429\n",
      "Average FMeasure: 0.09568954089746394\n",
      "rL\n",
      "Average Precision: 0.2061267193940587\n",
      "Average Recall: 0.19324685419277968\n",
      "Average FMeasure: 0.187871721475997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjsci\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 2000 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.30582607105851406\n",
      "Average Recall: 0.28777285664917884\n",
      "Average FMeasure: 0.28064585492356864\n",
      "r2\n",
      "Average Precision: 0.0986452138136795\n",
      "Average Recall: 0.09302416553759871\n",
      "Average FMeasure: 0.09056332576675309\n",
      "rL\n",
      "Average Precision: 0.20120788833075468\n",
      "Average Recall: 0.18887893273117612\n",
      "Average FMeasure: 0.1839384800908994\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3068565226864782\n",
      "Average Recall: 0.28657027609628977\n",
      "Average FMeasure: 0.2807813193872062\n",
      "r2\n",
      "Average Precision: 0.09925040543238371\n",
      "Average Recall: 0.093110199053022\n",
      "Average FMeasure: 0.09092292044806086\n",
      "rL\n",
      "Average Precision: 0.20212777018592507\n",
      "Average Recall: 0.18803017774216838\n",
      "Average FMeasure: 0.18404669663971535\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3043886530215791\n",
      "Average Recall: 0.28737351843261555\n",
      "Average FMeasure: 0.280177819925041\n",
      "r2\n",
      "Average Precision: 0.09736584470009543\n",
      "Average Recall: 0.09245708452654036\n",
      "Average FMeasure: 0.08986440207245577\n",
      "rL\n",
      "Average Precision: 0.2001716210419416\n",
      "Average Recall: 0.18830806526369556\n",
      "Average FMeasure: 0.18344982250940192\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3045745850083928\n",
      "Average Recall: 0.28739638385723315\n",
      "Average FMeasure: 0.28032746267271497\n",
      "r2\n",
      "Average Precision: 0.09775849297966675\n",
      "Average Recall: 0.09253847078355366\n",
      "Average FMeasure: 0.09006320149777466\n",
      "rL\n",
      "Average Precision: 0.2004539213680205\n",
      "Average Recall: 0.1885424642415465\n",
      "Average FMeasure: 0.18371441473547662\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.30397267187124755\n",
      "Average Recall: 0.28619139817374456\n",
      "Average FMeasure: 0.27912244666166103\n",
      "r2\n",
      "Average Precision: 0.09706951203197081\n",
      "Average Recall: 0.09169964309317838\n",
      "Average FMeasure: 0.08912649723786037\n",
      "rL\n",
      "Average Precision: 0.20018417049302165\n",
      "Average Recall: 0.1877458196035604\n",
      "Average FMeasure: 0.1829250196748344\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3043951820017956\n",
      "Average Recall: 0.2863060001639199\n",
      "Average FMeasure: 0.27929240057582977\n",
      "r2\n",
      "Average Precision: 0.097132506677442\n",
      "Average Recall: 0.09168024088832957\n",
      "Average FMeasure: 0.08914852804034963\n",
      "rL\n",
      "Average Precision: 0.20044208697875684\n",
      "Average Recall: 0.18788370438730137\n",
      "Average FMeasure: 0.18307463067589996\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3028953429946249\n",
      "Average Recall: 0.2855301860780199\n",
      "Average FMeasure: 0.27837701600435194\n",
      "r2\n",
      "Average Precision: 0.09591045449352333\n",
      "Average Recall: 0.09076986409360889\n",
      "Average FMeasure: 0.08823976855238715\n",
      "rL\n",
      "Average Precision: 0.19930964629940778\n",
      "Average Recall: 0.18725555394189589\n",
      "Average FMeasure: 0.18237745262561314\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3041431399857756\n",
      "Average Recall: 0.28610213633315446\n",
      "Average FMeasure: 0.2792114077577502\n",
      "r2\n",
      "Average Precision: 0.0966236921468465\n",
      "Average Recall: 0.0911542484086482\n",
      "Average FMeasure: 0.08876231566284447\n",
      "rL\n",
      "Average Precision: 0.20003673396810648\n",
      "Average Recall: 0.1874167000858242\n",
      "Average FMeasure: 0.1827861134005106\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.3050728230253715\n",
      "Average Recall: 0.28611992005373366\n",
      "Average FMeasure: 0.279668613398882\n",
      "r2\n",
      "Average Precision: 0.09699896700446646\n",
      "Average Recall: 0.09124382909672234\n",
      "Average FMeasure: 0.08900542378570851\n",
      "rL\n",
      "Average Precision: 0.20057289469405504\n",
      "Average Recall: 0.18734048597124414\n",
      "Average FMeasure: 0.18301913232048853\n"
     ]
    }
   ],
   "source": [
    "def nnmf():    \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary.\n",
    "        summary = []\n",
    "        id2score = {}\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores.\n",
    "        tf_idf = vectorizer.fit_transform(sentences)\n",
    "        # Non-negative matrix factorization.\n",
    "        nnmf = NMF(n_components=12, init='random', random_state=0, max_iter=2000)\n",
    "        factors = nnmf.fit_transform(tf_idf)\n",
    "        # Sum each sentence's factor values.\n",
    "        for i in range(factors.shape[0]):\n",
    "            score = 0\n",
    "            for j in range(factors.shape[1]):\n",
    "                score += factors[i,j]\n",
    "            id2score[i] = score\n",
    "        # Selecting K highest-scoring sentences for summarization.\n",
    "        for k in range(sentence_count):\n",
    "            sentence_id = max(id2score.items(), key=operator.itemgetter(1))[0]\n",
    "            id2score.pop(sentence_id)\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "    \n",
    "nnmf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e375ea",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 5</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF K-Means Clustering</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. Each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, each sentence can be thought of as a vector of size |V| where each cell is the TF-IDF value of a word in that sentence. These vectors can be used to obtain cosine similarities and cluster sentences into K groups, where K is the number of sentences desired in the final article summarization. The sentence closest to the center of each of these clusters will be chosen for the summary. In this way, sentences will avoid overlap in their textual and semantic similarities, instead covering a larger swath of the original article's meaning. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "8493e08f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3756438151973616\n",
      "Average Recall: 0.3495646051299048\n",
      "Average FMeasure: 0.3430318528861402\n",
      "r2\n",
      "Average Precision: 0.14584689192474393\n",
      "Average Recall: 0.1333401863744245\n",
      "Average FMeasure: 0.13150890416070668\n",
      "rL\n",
      "Average Precision: 0.24063836895979043\n",
      "Average Recall: 0.2227905409449432\n",
      "Average FMeasure: 0.21849746867633482\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.37560320477686737\n",
      "Average Recall: 0.34741529917141856\n",
      "Average FMeasure: 0.34286658504353257\n",
      "r2\n",
      "Average Precision: 0.14306554016733325\n",
      "Average Recall: 0.13096963878768192\n",
      "Average FMeasure: 0.1295046671922906\n",
      "rL\n",
      "Average Precision: 0.23950772936772302\n",
      "Average Recall: 0.22077739878055283\n",
      "Average FMeasure: 0.21756647027569642\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3768218116547009\n",
      "Average Recall: 0.34700420182878466\n",
      "Average FMeasure: 0.3428320971945409\n",
      "r2\n",
      "Average Precision: 0.14303867735135747\n",
      "Average Recall: 0.13063095824483087\n",
      "Average FMeasure: 0.1292741123206082\n",
      "rL\n",
      "Average Precision: 0.24009105550816248\n",
      "Average Recall: 0.22046180438665275\n",
      "Average FMeasure: 0.21753810004143634\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.37420252524261116\n",
      "Average Recall: 0.34554785783246483\n",
      "Average FMeasure: 0.34104134593830404\n",
      "r2\n",
      "Average Precision: 0.1408103080043666\n",
      "Average Recall: 0.12891668310996354\n",
      "Average FMeasure: 0.12752708270289806\n",
      "rL\n",
      "Average Precision: 0.23815334302819655\n",
      "Average Recall: 0.2192420866865446\n",
      "Average FMeasure: 0.2161481324860973\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3726973667019643\n",
      "Average Recall: 0.3437655561447104\n",
      "Average FMeasure: 0.33956779724168307\n",
      "r2\n",
      "Average Precision: 0.1406231662623876\n",
      "Average Recall: 0.1283536526181505\n",
      "Average FMeasure: 0.12718259678779878\n",
      "rL\n",
      "Average Precision: 0.2379297626498561\n",
      "Average Recall: 0.21847969482675358\n",
      "Average FMeasure: 0.2157384489059626\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.37306229373412414\n",
      "Average Recall: 0.34278652674840016\n",
      "Average FMeasure: 0.33923626815275876\n",
      "r2\n",
      "Average Precision: 0.14087526745153361\n",
      "Average Recall: 0.12795361193927865\n",
      "Average FMeasure: 0.1271629885290533\n",
      "rL\n",
      "Average Precision: 0.23806563718503757\n",
      "Average Recall: 0.21781247349666769\n",
      "Average FMeasure: 0.2154963094627257\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3738074438399877\n",
      "Average Recall: 0.3429870535274269\n",
      "Average FMeasure: 0.33955993198322204\n",
      "r2\n",
      "Average Precision: 0.14159414133448747\n",
      "Average Recall: 0.12839596554613483\n",
      "Average FMeasure: 0.127658513699439\n",
      "rL\n",
      "Average Precision: 0.23816656676086448\n",
      "Average Recall: 0.2177511040778812\n",
      "Average FMeasure: 0.2154374465715597\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.37331882364522806\n",
      "Average Recall: 0.3432572572637198\n",
      "Average FMeasure: 0.3396589270135528\n",
      "r2\n",
      "Average Precision: 0.14128924588914302\n",
      "Average Recall: 0.12837724531194794\n",
      "Average FMeasure: 0.1276002110581594\n",
      "rL\n",
      "Average Precision: 0.2374816211038288\n",
      "Average Recall: 0.21767420049475328\n",
      "Average FMeasure: 0.2152284771606689\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3734509055488181\n",
      "Average Recall: 0.34391125301883635\n",
      "Average FMeasure: 0.34008259962122095\n",
      "r2\n",
      "Average Precision: 0.14150872804562276\n",
      "Average Recall: 0.12866250934336343\n",
      "Average FMeasure: 0.12788380929885623\n",
      "rL\n",
      "Average Precision: 0.237080496244489\n",
      "Average Recall: 0.217493231115858\n",
      "Average FMeasure: 0.21499784112019896\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.37368484563488524\n",
      "Average Recall: 0.3437398840099932\n",
      "Average FMeasure: 0.3400570813044879\n",
      "r2\n",
      "Average Precision: 0.14155658002473417\n",
      "Average Recall: 0.1284667199735181\n",
      "Average FMeasure: 0.1277818272749966\n",
      "rL\n",
      "Average Precision: 0.23718887835120692\n",
      "Average Recall: 0.21716370996080378\n",
      "Average FMeasure: 0.2148370678889909\n"
     ]
    }
   ],
   "source": [
    "def tfidfkmeans():  \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary.\n",
    "        summary = []\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores.\n",
    "        tf_idf = vectorizer.fit_transform(sentences)\n",
    "        # K-Means clustering.\n",
    "        kmeans = KMeans(n_clusters=sentence_count, random_state=0)\n",
    "        # Compute K clusters.\n",
    "        sentence_ids = []\n",
    "        kmeans.fit(tf_idf)\n",
    "        # For each of K sets of points, one per cluster, compute the closest to cluster center.\n",
    "        for i in range(kmeans.n_clusters):\n",
    "            tf_idf_clustered = tf_idf[kmeans.labels_ == i]\n",
    "            sentence_id, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, tf_idf_clustered)\n",
    "            sentence_ids.append(sentence_id[i])\n",
    "        # Find actual sentence ids from original article.\n",
    "        for i in range(len(sentence_ids)):\n",
    "            count = sentence_ids[i]\n",
    "            sentence_ids[i] = [j for j, n in enumerate(kmeans.labels_) if n == i][count]\n",
    "        # Selecting sentences for summarization.\n",
    "        for sentence_id in sentence_ids:\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "    \n",
    "tfidfkmeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96d9b9",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Method 6</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">TF-IDF Non-negative Matrix Factorization K-Means Clustering</p>\n",
    "---\n",
    "\n",
    "This method involves gathering the TF-IDF scores of each word in the vocabulary of an article in relation to each document of the article. Each sentence is considered its own \"document\" within the article. Once the TF-IDF matrix of size |D| x |V| is calculated, non-negative matrix factorization can be used to reduce the matrix into two matrices of lesser dimensionality of size |D| x F and F x |V| where F is the number of latent features present in the article. The |D| x F matrix can be used to obtain cosine similarities and cluster sentences into K groups, where K is the number of sentences desired in the final article summarization. For each cluster, a member of that cluster closest to the center of the cluster will be chosen for the summary. In this way, sentences will avoid overlap in their textual and semantic similarities, instead covering a larger swath of the original article's meaning. Blank sentences are removed prior to TFIDF embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85746e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.301569889181544\n",
      "Average Recall: 0.2595424970759193\n",
      "Average FMeasure: 0.26284177220883653\n",
      "r2\n",
      "Average Precision: 0.08901698102077771\n",
      "Average Recall: 0.07542069399586723\n",
      "Average FMeasure: 0.07667078708739174\n",
      "rL\n",
      "Average Precision: 0.19581709298510488\n",
      "Average Recall: 0.1674488494349567\n",
      "Average FMeasure: 0.16957997437452516\n",
      "2000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.30016572852206375\n",
      "Average Recall: 0.25312418961119976\n",
      "Average FMeasure: 0.2588922889823209\n",
      "r2\n",
      "Average Precision: 0.08583047950309097\n",
      "Average Recall: 0.07193066178444159\n",
      "Average FMeasure: 0.07349417734347045\n",
      "rL\n",
      "Average Precision: 0.19440044864685865\n",
      "Average Recall: 0.16275301714098828\n",
      "Average FMeasure: 0.16652453321361554\n",
      "3000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.3014261497655132\n",
      "Average Recall: 0.2538877219842601\n",
      "Average FMeasure: 0.260040776504014\n",
      "r2\n",
      "Average Precision: 0.08625405721676663\n",
      "Average Recall: 0.0725493852396443\n",
      "Average FMeasure: 0.07418869736946586\n",
      "rL\n",
      "Average Precision: 0.195341053273953\n",
      "Average Recall: 0.1634729998133325\n",
      "Average FMeasure: 0.16754015972522088\n",
      "4000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2985258018653552\n",
      "Average Recall: 0.2544408752627963\n",
      "Average FMeasure: 0.25951525310497836\n",
      "r2\n",
      "Average Precision: 0.08471854064381182\n",
      "Average Recall: 0.07256566211337137\n",
      "Average FMeasure: 0.07378308332655945\n",
      "rL\n",
      "Average Precision: 0.19227451293870126\n",
      "Average Recall: 0.16288924707200061\n",
      "Average FMeasure: 0.16624235792784414\n",
      "5000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2972644283266819\n",
      "Average Recall: 0.25375616824500197\n",
      "Average FMeasure: 0.2587651903030371\n",
      "r2\n",
      "Average Precision: 0.08384086607177652\n",
      "Average Recall: 0.07194792635418568\n",
      "Average FMeasure: 0.07307678047431475\n",
      "rL\n",
      "Average Precision: 0.19155994432147433\n",
      "Average Recall: 0.16244302578098346\n",
      "Average FMeasure: 0.16579652642365197\n",
      "6000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.2963479739368144\n",
      "Average Recall: 0.2520726731189011\n",
      "Average FMeasure: 0.2573564725766828\n",
      "r2\n",
      "Average Precision: 0.08302385855199816\n",
      "Average Recall: 0.07068370237540425\n",
      "Average FMeasure: 0.07202343625009397\n",
      "rL\n",
      "Average Precision: 0.1909297515592429\n",
      "Average Recall: 0.1611981275806904\n",
      "Average FMeasure: 0.16480173592394576\n",
      "7000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29685660686759385\n",
      "Average Recall: 0.2517115153352765\n",
      "Average FMeasure: 0.2571860961121134\n",
      "r2\n",
      "Average Precision: 0.0830682796253619\n",
      "Average Recall: 0.07071294444223436\n",
      "Average FMeasure: 0.07196880028648481\n",
      "rL\n",
      "Average Precision: 0.1912346651662524\n",
      "Average Recall: 0.16105935726742165\n",
      "Average FMeasure: 0.1647267857139313\n",
      "8000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29650016205701163\n",
      "Average Recall: 0.25173229456811386\n",
      "Average FMeasure: 0.25710020808460515\n",
      "r2\n",
      "Average Precision: 0.08325336788299206\n",
      "Average Recall: 0.07091820447016482\n",
      "Average FMeasure: 0.07213936930572327\n",
      "rL\n",
      "Average Precision: 0.19103852016200523\n",
      "Average Recall: 0.1610365110846542\n",
      "Average FMeasure: 0.16464991583369476\n",
      "9000 articles summarized.\n",
      "r1\n",
      "Average Precision: 0.29706676594447473\n",
      "Average Recall: 0.2520620528042042\n",
      "Average FMeasure: 0.2576428131001921\n",
      "r2\n",
      "Average Precision: 0.0835361135478799\n",
      "Average Recall: 0.0711488380942451\n",
      "Average FMeasure: 0.07243425914130237\n",
      "rL\n",
      "Average Precision: 0.19124534126798712\n",
      "Average Recall: 0.1611376368692555\n",
      "Average FMeasure: 0.1648887955283413\n",
      "\n",
      "=====\n",
      "Final scores.\n",
      "r1\n",
      "Average Precision: 0.29778802349448347\n",
      "Average Recall: 0.25201470087884\n",
      "Average FMeasure: 0.25804232483535894\n",
      "r2\n",
      "Average Precision: 0.08356617741488814\n",
      "Average Recall: 0.07091115507497997\n",
      "Average FMeasure: 0.07236128688857857\n",
      "rL\n",
      "Average Precision: 0.19147767968502022\n",
      "Average Recall: 0.1609210936851851\n",
      "Average FMeasure: 0.16495082364251376\n"
     ]
    }
   ],
   "source": [
    "def nnmfkmeans():  \n",
    "    # Evaluation values.\n",
    "    stats = ['r1', 'r2', 'rL']\n",
    "    avg_precision = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_recall = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    avg_fmeasure = {\n",
    "        'r1' : 0,\n",
    "        'r2' : 0,\n",
    "        'rL' : 0\n",
    "    }\n",
    "    # Vectorizer.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=False)\n",
    "    # Summarize every article.\n",
    "    for a, article in enumerate(test.loc[0:9999, \"article\"]):\n",
    "        # Counter.\n",
    "        if (a % 1000 == 0 and a > 0):\n",
    "            print(f'{a} articles summarized.')\n",
    "            for stat in stats:\n",
    "                print(f'{stat}')\n",
    "                print(f'Average Precision: {avg_precision[stat]/(a)}')\n",
    "                print(f'Average Recall: {avg_recall[stat]/(a)}')\n",
    "                print(f'Average FMeasure: {avg_fmeasure[stat]/(a)}')\n",
    "        # Initial blank summary.\n",
    "        summary = []\n",
    "        # Tokenize the article by sentences.\n",
    "        sentences = sent_tokenize(article)\n",
    "        final = sent_tokenize(article)\n",
    "        # For storing indices of blank sentences.\n",
    "        blanks = []\n",
    "        # Clean each sentence.\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = clean(sentence)\n",
    "            if sentences[i] == \"\" or sentences[i] == \" \":\n",
    "                blanks.append(i)\n",
    "        # Remove blank sentences.\n",
    "        for i in range(len(blanks) - 1, -1, -1):\n",
    "            sentences.pop(blanks[i])\n",
    "            final.pop(blanks[i])\n",
    "        # Setting summary sentence count.\n",
    "        sentence_count = min(max_sentence_count, len(sentences))\n",
    "        # Calculate TF-IDF scores.\n",
    "        tf_idf = vectorizer.fit_transform(sentences)\n",
    "        # Non-negative matrix factorization.\n",
    "        nnmf = NMF(n_components=10, init='random', random_state=0, max_iter=2000)\n",
    "        factors = nnmf.fit_transform(tf_idf)\n",
    "        # K-Means clustering.\n",
    "        kmeans = KMeans(n_clusters=sentence_count, random_state=0)\n",
    "        # Compute K clusters.\n",
    "        sentence_ids = []\n",
    "        kmeans.fit(factors)\n",
    "        # For each of K sets of points, one per cluster, compute the closest to cluster center.\n",
    "        for i in range(kmeans.n_clusters):\n",
    "            factors_clustered = factors[kmeans.labels_ == i]\n",
    "            sentence_id, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, factors_clustered)\n",
    "            sentence_ids.append(sentence_id[i])\n",
    "        # Find actual sentence ids from original article.\n",
    "        for i in range(len(sentence_ids)):\n",
    "            count = sentence_ids[i]\n",
    "            sentence_ids[i] = [j for j, n in enumerate(kmeans.labels_) if n == i][count]\n",
    "        # Selecting sentences for summarization.\n",
    "        for sentence_id in sentence_ids:\n",
    "            summary.append(final[sentence_id])\n",
    "        candidate = ' '.join(summary)\n",
    "        reference = test.loc[a,\"highlights\"]\n",
    "        # Scoring.\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        for stat, key in zip(stats, scores):\n",
    "            avg_precision[stat] += scores[key][0]\n",
    "            avg_recall[stat] += scores[key][1]\n",
    "            avg_fmeasure[stat] += scores[key][2]\n",
    "    for stat in stats:\n",
    "        avg_precision[stat] /= (a+1)\n",
    "        avg_recall[stat] /= (a+1)\n",
    "        avg_fmeasure[stat] /= (a+1)\n",
    "    print(\"\\n=====\")\n",
    "    print(\"Final scores.\")\n",
    "    for stat in stats:\n",
    "        print(f'{stat}')\n",
    "        print(f'Average Precision: {avg_precision[stat]}')\n",
    "        print(f'Average Recall: {avg_recall[stat]}')\n",
    "        print(f'Average FMeasure: {avg_fmeasure[stat]}')\n",
    "    \n",
    "nnmfkmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0e55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
